#!/usr/bin/env python3
"""
Test script for MLX-GUI complete workflow:
1. Discover models
2. Install a model
3. Load the model  
4. Use chat completions
"""

import asyncio
import httpx
import json
import time
from typing import Dict, Any

BASE_URL = "http://localhost:8000"

async def test_workflow():
    """Test the complete MLX-GUI workflow."""
    
    async with httpx.AsyncClient(timeout=300.0) as client:
        print("ğŸ§ª Testing MLX-GUI Complete Workflow\n")
        
        # Step 1: Check system status
        print("1ï¸âƒ£ Checking system status...")
        try:
            response = await client.get(f"{BASE_URL}/v1/system/status")
            if response.status_code == 200:
                status = response.json()
                print(f"   âœ… System: {status['system']['platform']} {status['system']['architecture']}")
                print(f"   âœ… MLX Compatible: {status['mlx_compatible']}")
                print(f"   âœ… Memory: {status['system']['memory']['available_gb']:.1f}GB available")
            else:
                print(f"   âŒ System status check failed: {response.status_code}")
                return
        except Exception as e:
            print(f"   âŒ Error checking system status: {e}")
            return
        
        # Step 2: Discover compatible models
        print("\n2ï¸âƒ£ Discovering compatible models...")
        try:
            response = await client.get(f"{BASE_URL}/v1/discover/compatible?limit=5")
            if response.status_code == 200:
                discovery = response.json()
                models = discovery['models']
                print(f"   âœ… Found {len(models)} compatible models:")
                for model in models[:3]:  # Show first 3
                    print(f"      - {model['id']} ({model['estimated_memory_gb']:.1f}GB)")
                
                # Choose a model for testing
                if models:
                    test_model = models[0]  # Use first compatible model
                    model_id = test_model['id']
                    print(f"   ğŸ“‹ Selected for testing: {model_id}")
                else:
                    print("   âŒ No compatible models found")
                    return
            else:
                print(f"   âŒ Model discovery failed: {response.status_code}")
                return
        except Exception as e:
            print(f"   âŒ Error discovering models: {e}")
            return
        
        # Step 3: Install the model
        print(f"\n3ï¸âƒ£ Installing model: {model_id}")
        try:
            install_data = {
                "model_id": model_id,
                "name": None  # Use default name
            }
            response = await client.post(f"{BASE_URL}/v1/models/install", json=install_data)
            if response.status_code == 200:
                install_result = response.json()
                model_name = install_result['model_name']
                print(f"   âœ… Model installed: {model_name}")
                print(f"   ğŸ“Š Estimated memory: {install_result['estimated_memory_gb']:.1f}GB")
            else:
                result = response.json()
                print(f"   âŒ Installation failed: {result.get('detail', 'Unknown error')}")
                return
        except Exception as e:
            print(f"   âŒ Error installing model: {e}")
            return
        
        # Step 4: Check installed models (OpenAI format)
        print(f"\n4ï¸âƒ£ Checking installed models...")
        try:
            response = await client.get(f"{BASE_URL}/v1/models")
            if response.status_code == 200:
                models_list = response.json()
                print(f"   âœ… Found {len(models_list['data'])} installed models:")
                for model in models_list['data']:
                    print(f"      - {model['id']}")
            else:
                print(f"   âŒ Failed to list models: {response.status_code}")
        except Exception as e:
            print(f"   âŒ Error listing models: {e}")
        
        # Step 5: Test chat completions
        print(f"\n5ï¸âƒ£ Testing chat completions with {model_name}...")
        try:
            chat_data = {
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful AI assistant running on Apple Silicon with MLX."
                    },
                    {
                        "role": "user", 
                        "content": "Hello! Can you tell me what you are and how you're running?"
                    }
                ],
                "max_tokens": 100,
                "temperature": 0.7
            }
            
            print("   ğŸ”„ Sending chat completion request...")
            start_time = time.time()
            response = await client.post(f"{BASE_URL}/v1/chat/completions", json=chat_data)
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                assistant_message = result['choices'][0]['message']['content']
                usage = result['usage']
                
                print(f"   âœ… Chat completion successful!")
                print(f"   ğŸ¤– Assistant: {assistant_message}")
                print(f"   ğŸ“Š Tokens: {usage['prompt_tokens']} prompt + {usage['completion_tokens']} completion = {usage['total_tokens']} total")
                print(f"   â±ï¸  Time: {end_time - start_time:.2f}s")
                
                # Calculate tokens per second
                if end_time > start_time:
                    tps = usage['completion_tokens'] / (end_time - start_time)
                    print(f"   ğŸš€ Speed: {tps:.1f} tokens/second")
                
            else:
                result = response.json()
                print(f"   âŒ Chat completion failed: {result.get('detail', 'Unknown error')}")
                return
                
        except Exception as e:
            print(f"   âŒ Error in chat completion: {e}")
            return
        
        # Step 6: Test another chat message (conversation)
        print(f"\n6ï¸âƒ£ Testing follow-up message...")
        try:
            chat_data = {
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful AI assistant."
                    },
                    {
                        "role": "user",
                        "content": "What is 2 + 2?"
                    }
                ],
                "max_tokens": 50,
                "temperature": 0.1
            }
            
            response = await client.post(f"{BASE_URL}/v1/chat/completions", json=chat_data)
            if response.status_code == 200:
                result = response.json()
                assistant_message = result['choices'][0]['message']['content']
                print(f"   âœ… Follow-up successful!")
                print(f"   ğŸ¤– Assistant: {assistant_message}")
            else:
                print(f"   âŒ Follow-up failed: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ Error in follow-up: {e}")
        
        # Step 7: Check model status
        print(f"\n7ï¸âƒ£ Checking model status...")
        try:
            response = await client.get(f"{BASE_URL}/v1/manager/models/{model_name}/status")
            if response.status_code == 200:
                status = response.json()
                print(f"   âœ… Model status: {status['status']}")
                print(f"   ğŸ“Š Memory usage: {status['memory_usage_gb']:.1f}GB")
                print(f"   â° Last used: {status['last_used_at']}")
            else:
                print(f"   âŒ Status check failed: {response.status_code}")
        except Exception as e:
            print(f"   âŒ Error checking status: {e}")
        
        print(f"\nğŸ‰ Workflow test completed successfully!")
        print(f"ğŸ“‹ Summary:")
        print(f"   - Model: {model_name}")
        print(f"   - Source: {model_id}")
        print(f"   - Status: Loaded and responding")
        print(f"   - API: OpenAI-compatible /v1/chat/completions")


if __name__ == "__main__":
    asyncio.run(test_workflow())